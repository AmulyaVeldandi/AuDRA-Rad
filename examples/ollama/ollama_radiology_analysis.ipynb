{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Radiology Report Analysis with Ollama (Local Llama 3.1 8B)\n",
    "\n",
    "This notebook demonstrates how to use Ollama running locally to analyze MIMIC-IV radiology reports for pulmonary nodules according to Fleischner guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "from src.services.ollama_llm import OllamaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama client\n",
    "# Make sure Ollama is running: ollama serve\n",
    "# And model is pulled: ollama pull llama3.1:8b\n",
    "\n",
    "client = OllamaClient()\n",
    "print(f\"Using model: {client.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIMIC-IV radiology data\n",
    "df = pd.read_csv('/Users/amulyaveldandi/Downloads/mimic-iv-note-deidentified-free-text-clinical-notes-2.2/note/radiology.csv')\n",
    "print(f\"Total radiology reports: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for CT Chest reports with pulmonary nodules\n",
    "ct_chest = df[df['text'].str.contains('CT CHEST|CT OF THE CHEST|CHEST CT', case=False, na=False)]\n",
    "print(f\"Total CT Chest reports: {len(ct_chest)}\")\n",
    "\n",
    "nodule_terms = 'nodule|nodular|pulmonary nodule'\n",
    "ct_chest_nodules = ct_chest[ct_chest['text'].str.contains(nodule_terms, case=False, na=False)]\n",
    "print(f\"CT Chest reports with nodules: {len(ct_chest_nodules)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample report\n",
    "if len(ct_chest_nodules) > 0:\n",
    "    sample_report = ct_chest_nodules.iloc[0]['text']\n",
    "    print(\"Sample CT Chest report with nodule:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(sample_report[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ollama to extract nodule information\n",
    "prompt = f\"\"\"\n",
    "Analyze this radiology report and extract pulmonary nodule information:\n",
    "\n",
    "{sample_report}\n",
    "\n",
    "Extract the following information:\n",
    "1. Number of nodules mentioned\n",
    "2. Size of each nodule (in mm)\n",
    "3. Location of nodules\n",
    "4. Nodule characteristics (solid, ground-glass, part-solid)\n",
    "5. Any follow-up recommendations mentioned\n",
    "\n",
    "Provide a concise summary.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analyzing report with Llama 3.1 8B...\\n\")\n",
    "response = client.generate(prompt, temperature=0.1, max_tokens=512)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structured JSON with Fleischner guideline recommendations\n",
    "fleischner_prompt = f\"\"\"\n",
    "Analyze this CT chest report for pulmonary nodules and provide Fleischner guideline recommendations:\n",
    "\n",
    "{sample_report}\n",
    "\n",
    "Return a JSON object with:\n",
    "{{\n",
    "  \"nodules_found\": true/false,\n",
    "  \"nodule_count\": number,\n",
    "  \"largest_nodule_size_mm\": number or null,\n",
    "  \"nodule_type\": \"solid\" or \"ground-glass\" or \"part-solid\" or \"mixed\" or null,\n",
    "  \"fleischner_applicable\": true/false,\n",
    "  \"recommended_followup\": \"string describing follow-up interval\",\n",
    "  \"confidence\": \"high\" or \"medium\" or \"low\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting structured data...\\n\")\n",
    "json_response = client.generate_json(fleischner_prompt)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple reports\n",
    "def analyze_nodule_report(report_text: str) -> dict:\n",
    "    \"\"\"Analyze a radiology report for nodule information.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyze this radiology report for pulmonary nodules:\n",
    "\n",
    "{report_text[:2000]}  # Limit to first 2000 chars\n",
    "\n",
    "Return JSON with:\n",
    "{{\n",
    "  \"has_nodules\": true/false,\n",
    "  \"nodule_size_mm\": number or null,\n",
    "  \"followup_needed\": true/false\n",
    "}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        return client.generate_json(prompt)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process first 5 reports (you can increase this)\n",
    "sample_size = 5\n",
    "results = []\n",
    "\n",
    "print(f\"Processing {sample_size} reports...\\n\")\n",
    "for idx, row in ct_chest_nodules.head(sample_size).iterrows():\n",
    "    print(f\"Processing report {idx}...\")\n",
    "    result = analyze_nodule_report(row['text'])\n",
    "    result['note_id'] = row['note_id']\n",
    "    results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nResults:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ollama client metrics\n",
    "print(f\"Total tokens used: {client.total_tokens}\")\n",
    "print(f\"Total API calls: {client._total_calls}\")\n",
    "print(f\"Error rate: {client.error_rate:.2%}\")\n",
    "if client.latencies_ms:\n",
    "    print(f\"Average latency: {sum(client.latencies_ms)/len(client.latencies_ms):.2f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
